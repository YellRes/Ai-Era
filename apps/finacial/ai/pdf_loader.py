"""
PDF æ•°æ®åŠ è½½å·¥å…·é›†
åŒ…å« PDF æ–‡ä»¶åŠ è½½ã€è§£æå’Œæ•°æ®æå–ç›¸å…³çš„å·¥å…·å‡½æ•°
"""

import os
import re
from langchain_core.tools import tool
from langchain_core.documents import Document
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings


# å…¨å±€å˜é‡ï¼šå­˜å‚¨åŠ è½½çš„PDFå‘é‡æ•°æ®åº“
pdf_vectorstore = None
pdf_content = None


def format_amount(value: float) -> str:
    """
    æ ¼å¼åŒ–é‡‘é¢,è‡ªåŠ¨è½¬æ¢ä¸ºä¸‡/äº¿å•ä½
    
    Args:
        value: é‡‘é¢æ•°å€¼
    
    Returns:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    abs_value = abs(value)
    sign = "-" if value < 0 else ""
    
    if abs_value >= 100_000_000:  # å¤§äºç­‰äº1äº¿
        return f"{sign}{abs_value / 100_000_000:.2f} äº¿å…ƒ"
    elif abs_value >= 10_000:  # å¤§äºç­‰äº1ä¸‡
        return f"{sign}{abs_value / 10_000:.2f} ä¸‡å…ƒ"
    else:
        return f"{sign}{abs_value:.2f} å…ƒ"

@tool
def extract_financial_metrics(query: str = "all") -> str:
    """
    ä»å·²åŠ è½½çš„è´¢åŠ¡æŠ¥è¡¨ PDF ä¸­æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼ˆè¥ä¸šæ”¶å…¥ã€å‡€åˆ©æ¶¦ã€èµ„äº§çŠ¶å†µç­‰ï¼‰ã€‚
    
    Args:
        query: æå–æ¨¡å¼ï¼Œé»˜è®¤ä¸º "all"ã€‚
    
    Returns:
        åŒ…å«è´¢åŠ¡æŒ‡æ ‡çš„æ ¼å¼åŒ–æŠ¥å‘Šã€‚
    """
    global pdf_content
    
    if pdf_content is None:
        return "âŒ è¯·å…ˆä½¿ç”¨ load_financial_pdf å·¥å…·åŠ è½½ PDF æ–‡ä»¶"
    
    # å®šä¹‰è¦æå–çš„è´¢åŠ¡æŒ‡æ ‡åŠå…¶æ­£åˆ™æ¨¡å¼
    patterns = {
        # åˆ©æ¶¦è¡¨æŒ‡æ ‡ (æ¯ä¸ªæ±‰å­—ä¹‹é—´éƒ½å…è®¸ç©ºç™½ç¬¦)
        "è¥ä¸šæ”¶å…¥": r"è¥[\s\n]*ä¸š[\s\n]*(?:æ€»[\s\n]*)?æ”¶[\s\n]*å…¥[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*([\d,ï¼Œ]+\.?\d*)",
        "åˆ©æ¶¦æ€»é¢": r"åˆ©[\s\n]*æ¶¦[\s\n]*æ€»[\s\n]*é¢[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*([\d,ï¼Œ]+\.?\d*)",
        "å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦": r"å½’[\s\n]*å±[\s\n]*äº[\s\n]*ä¸Š[\s\n]*å¸‚[\s\n]*å…¬[\s\n]*å¸[\s\n]*è‚¡[\s\n]*ä¸œ[\s\n]*çš„?[\s\n]*å‡€[\s\n]*åˆ©[\s\n]*æ¶¦[\s\n]*[ï¼ˆ(]?[\s\n]*å…ƒ?[\s\n]*[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d,ï¼Œ]+\.?\d*)",
        "æ‰£éå‡€åˆ©æ¶¦": r"æ‰£[\s\n]*é™¤[\s\n]*é?[\s\n]*ç»[\s\n]*å¸¸[\s\n]*æ€§[\s\n]*æŸ[\s\n]*ç›Š[\s\n]*çš„?[\s\n]*å‡€[\s\n]*åˆ©[\s\n]*æ¶¦[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d,ï¼Œ]+\.?\d*)",
        
        # æ¯è‚¡æŒ‡æ ‡
        "åŸºæœ¬æ¯è‚¡æ”¶ç›Š": r"åŸº[\s\n]*æœ¬[\s\n]*æ¯[\s\n]*è‚¡[\s\n]*æ”¶[\s\n]*ç›Š[ï¼ˆ(]?å…ƒ/è‚¡[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d.]+)",
        "ç¨€é‡Šæ¯è‚¡æ”¶ç›Š": r"ç¨€[\s\n]*é‡Š[\s\n]*æ¯[\s\n]*è‚¡[\s\n]*æ”¶[\s\n]*ç›Š[ï¼ˆ(]?å…ƒ/è‚¡[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d.]+)",
        
        # èµ„äº§è´Ÿå€ºè¡¨æŒ‡æ ‡
        "æ€»èµ„äº§": r"æ€»[\s\n]*èµ„[\s\n]*äº§[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*([\d,ï¼Œ]+\.?\d*)",
        "å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰€æœ‰è€…æƒç›Š": r"å½’[\s\n]*å±[\s\n]*äº[\s\n]*ä¸Š[\s\n]*å¸‚[\s\n]*å…¬[\s\n]*å¸[\s\n]*è‚¡[\s\n]*ä¸œ[\s\n]*çš„?[\s\n]*æ‰€[\s\n]*æœ‰[\s\n]*è€…[\s\n]*æƒ[\s\n]*ç›Š[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*([\d,ï¼Œ]+\.?\d*)",
        
        # ç°é‡‘æµæŒ‡æ ‡
        "ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢": r"ç»[\s\n]*è¥[\s\n]*æ´»[\s\n]*åŠ¨[\s\n]*äº§[\s\n]*ç”Ÿ[\s\n]*çš„[\s\n]*ç°[\s\n]*é‡‘[\s\n]*æµ[\s\n]*é‡[\s\n]*å‡€[\s\n]*é¢[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d,ï¼Œ]+\.?\d*)",
        
        # æ”¶ç›Šç‡æŒ‡æ ‡
        "åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡": r"åŠ [\s\n]*æƒ[\s\n]*å¹³[\s\n]*å‡[\s\n]*å‡€[\s\n]*èµ„[\s\n]*äº§[\s\n]*æ”¶[\s\n]*ç›Š[\s\n]*ç‡[ï¼ˆ(]?%?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d.]+)%?",
    }
    
    # å®šä¹‰å“ªäº›æŒ‡æ ‡æ˜¯é‡‘é¢ç±»å‹(éœ€è¦æ ¼å¼åŒ–)
    amount_metrics = {
        "è¥ä¸šæ”¶å…¥", "åˆ©æ¶¦æ€»é¢", "å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦", 
        "æ‰£éå‡€åˆ©æ¶¦", "æ€»èµ„äº§", "å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰€æœ‰è€…æƒç›Š",
        "ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢"
    }
    
    result = "ğŸ“Š æå–çš„è´¢åŠ¡æŒ‡æ ‡ï¼š\n\n"
    found_any = False
    
    for name, pattern in patterns.items():
        match = re.search(pattern, pdf_content)
        if match:
            found_any = True
            value_str = match.group(1).replace(",", "").replace("ï¼Œ", "")
            try:
                value = float(value_str)
                if name in amount_metrics:
                    result += f"- {name}: {format_amount(value)}\n"
                else:
                    result += f"- {name}: {value}\n"
            except:
                result += f"- {name}: {value_str}\n"
                
    if not found_any:
        return "â“ æœªèƒ½åœ¨ PDF ä¸­æå–åˆ°å…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨æœç´¢ã€‚"
        
    return result


def extract_financial_table(page_content: str) -> list:
    """
    ä»å•é¡µå†…å®¹ä¸­æå–è¡¨æ ¼æ•°æ®
    
    Args:
        page_content: å•é¡µçš„æ–‡æœ¬å†…å®¹
    
    Returns:
        è¡¨æ ¼è¡Œæ•°æ®åˆ—è¡¨
    """
    rows = []
    lines = page_content.split('\n')
    
    for line in lines:
        # åŒ¹é…åŒ…å«æ•°å­—çš„è¡Œï¼ˆå¯èƒ½æ˜¯è¡¨æ ¼æ•°æ®ï¼‰
        # è´¢åŠ¡æŠ¥è¡¨ä¸­çš„æ•°æ®è¡Œé€šå¸¸åŒ…å«å¤šä¸ªæ•°å­—
        numbers = re.findall(r'-?[\d,ï¼Œ]+\.?\d*', line)
        if len(numbers) >= 2:  # è‡³å°‘æœ‰ä¸¤ä¸ªæ•°å­—æ‰è®¤ä¸ºæ˜¯è¡¨æ ¼è¡Œ
            # æå–è¡Œé¦–çš„é¡¹ç›®åç§°
            item_name = re.sub(r'[\d,ï¼Œ.%\-\s]+', '', line).strip()
            if item_name:
                rows.append({
                    "item": item_name,
                    "values": [n.replace(",", "").replace("ï¼Œ", "") for n in numbers]
                })
    
    return rows


def split_by_chinese_headers(text: str, source: str = "") -> list:
    """
    æŒ‰ä¸­æ–‡è´¢åŠ¡æŠ¥è¡¨æ ‡é¢˜åˆ†å‰²æ–‡æœ¬
    
    æ”¯æŒçš„æ ‡é¢˜æ ¼å¼ï¼š
    - ä¸€ã€äºŒã€ä¸‰ã€... ï¼ˆä¸­æ–‡æ•°å­—åºå·ï¼‰
    - ç¬¬ä¸€èŠ‚ã€ç¬¬äºŒèŠ‚ã€... ï¼ˆç« èŠ‚æ ¼å¼ï¼‰
    - ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ï¼ˆä¸‰ï¼‰... ï¼ˆæ‹¬å·æ ¼å¼ï¼‰
    - 1ã€2ã€3ã€... æˆ– 1.  2.  3. ï¼ˆé˜¿æ‹‰ä¼¯æ•°å­—åºå·ï¼‰
    
    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        source: æ¥æºæ–‡ä»¶è·¯å¾„
    
    Returns:
        åˆ†å‰²åçš„ Document åˆ—è¡¨
    """
    # åŒ¹é…ä¸­æ–‡è´¢åŠ¡æŠ¥è¡¨å¸¸è§çš„æ ‡é¢˜æ ¼å¼
    header_pattern = re.compile(
        r'^('
        r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€|'                    # ä¸€ã€äºŒã€ä¸‰ã€
        r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[èŠ‚ç« æ¡æ¬¾]|'        # ç¬¬ä¸€èŠ‚ã€ç¬¬äºŒç« 
        r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[)ï¼‰]|'          # ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰(1)(2)
        r'\d+[ã€.ï¼]\s*[^\d]|'                          # 1ã€ 2. 3ï¼
        r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]'                           # åœˆæ•°å­—
        r')',
        re.MULTILINE
    )
    
    # æ‰¾åˆ°æ‰€æœ‰æ ‡é¢˜çš„ä½ç½®
    matches = list(header_pattern.finditer(text))
    
    if not matches:
        # æ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªæ–‡æ¡£
        return [Document(page_content=text.strip(), metadata={"source": source, "header": "å…¨æ–‡"})]
    
    documents = []
    
    for i, match in enumerate(matches):
        start = match.start()
        # ä¸‹ä¸€ä¸ªæ ‡é¢˜çš„èµ·å§‹ä½ç½®ï¼Œæˆ–è€…æ–‡æœ¬æœ«å°¾
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        
        chunk_text = text[start:end].strip()
        
        if chunk_text:
            # æå–æ ‡é¢˜ä½œä¸ºå…ƒæ•°æ®
            header_text = match.group(1).strip()
            # è·å–æ ‡é¢˜åçš„ç¬¬ä¸€è¡Œä½œä¸ºå®Œæ•´æ ‡é¢˜
            first_line = chunk_text.split('\n')[0][:50]  # é™åˆ¶é•¿åº¦
            
            documents.append(Document(
                page_content=chunk_text,
                metadata={
                    "source": source,
                    "header": first_line,
                    "header_marker": header_text
                }
            ))
    
    # å¦‚æœç¬¬ä¸€ä¸ªæ ‡é¢˜ä¹‹å‰æœ‰å†…å®¹ï¼Œä¹Ÿä½œä¸ºä¸€ä¸ªæ–‡æ¡£
    if matches and matches[0].start() > 0:
        pre_content = text[:matches[0].start()].strip()
        if pre_content and len(pre_content) > 50:  # å¿½ç•¥å¤ªçŸ­çš„å‰ç½®å†…å®¹
            documents.insert(0, Document(
                page_content=pre_content,
                metadata={"source": source, "header": "æ–‡æ¡£å¼€å¤´"}
            ))
    
    return documents


def extract_number_from_text(text: str) -> list:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰æ•°å­—ï¼ˆæ”¯æŒåƒåˆ†ä½ã€å°æ•°ã€è´Ÿæ•°ã€å¸¦å•ä½ï¼‰
    
    Returns:
        [(æ•°å­—å€¼, åŸå§‹å­—ç¬¦ä¸²), ...]
    """
    # åŒ¹é…å„ç§æ•°å­—æ ¼å¼ï¼šè´Ÿæ•°ã€åƒåˆ†ä½ã€å°æ•°ã€å¸¦å•ä½ï¼ˆä¸‡ã€äº¿ã€å…ƒï¼‰
    patterns = [
        r'(-?[\d,ï¼Œ]+\.?\d*)\s*(?:ä¸‡å…ƒ|äº¿å…ƒ|å…ƒ)?',
        r'(-?[\d,ï¼Œ]+\.?\d*)',
    ]
    
    results = []
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                match = match[0]
            # æ¸…ç†æ ¼å¼
            clean_num = match.replace(',', '').replace('ï¼Œ', '').strip()
            if clean_num and clean_num not in ['-', '.']:
                try:
                    value = float(clean_num)
                    if abs(value) > 0:  # æ’é™¤0
                        results.append((value, match))
                except:
                    pass
    
    # å»é‡
    seen = set()
    unique_results = []
    for value, original in results:
        if value not in seen:
            seen.add(value)
            unique_results.append((value, original))
    
    return unique_results


@tool
def load_financial_pdf(pdf_path: str) -> str:
    """
    åŠ è½½å¹¶å¤„ç†è´¢åŠ¡æŠ¥è¡¨PDFæ–‡ä»¶ï¼ˆä¸­æ–‡ä¼˜åŒ–ç‰ˆï¼‰
    
    Args:
        pdf_path: PDFæ–‡ä»¶çš„è·¯å¾„
    
    Returns:
        åŠ è½½çŠ¶æ€ä¿¡æ¯
    """
    global pdf_vectorstore, pdf_content
    
    try:
        # ä½¿ç”¨ PyMuPDF åŠ è½½PDFï¼ˆå¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½ï¼‰
        print("ğŸ“‚ æ­£åœ¨åŠ è½½PDFæ–‡ä»¶...")
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        print(documents[0].page_content)
        print(f"âœ“ å·²åŠ è½½ {len(documents)} é¡µ")
        
        # ä¿å­˜åŸå§‹å†…å®¹
        pdf_content = "\n\n".join([doc.page_content for doc in documents])
        
        # æŒ‰æ ‡é¢˜åˆ†å‰²ï¼ˆä¿æŒè´¢åŠ¡æŠ¥è¡¨ç« èŠ‚å®Œæ•´æ€§ï¼‰
        print("ğŸ“ æ­£åœ¨æŒ‰æ ‡é¢˜åˆ†å‰²æ–‡æœ¬...")
        source = documents[0].metadata.get("source", pdf_path) if documents else pdf_path
        splits = split_by_chinese_headers(pdf_content, source)
        print(f"âœ“ æŒ‰æ ‡é¢˜åˆ†å‰²ï¼Œå…± {len(splits)} ä¸ªç« èŠ‚")
        
        # ä½¿ç”¨æœ¬åœ°ä¸­æ–‡ Embedding æ¨¡å‹åˆ›å»ºå‘é‡å­˜å‚¨
        try:
            print("ğŸ”§ æ­£åœ¨åŠ è½½ä¸­æ–‡ Embedding æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰...")
            embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-base-zh-v1.5",  # ä¸“é—¨çš„ä¸­æ–‡ Embedding æ¨¡å‹ï¼Œçº¦400MB
                model_kwargs={'device': 'cpu'},  # ä½¿ç”¨ CPUï¼Œå¦‚æœ‰ GPU å¯æ”¹ä¸º 'cuda'
                encode_kwargs={'normalize_embeddings': True}
            )
            
            print("ğŸ” æ­£åœ¨åˆ›å»ºå‘é‡ç´¢å¼•...")
            pdf_vectorstore = FAISS.from_documents(splits, embeddings)
            print("âœ“ å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆ")
            
            return f"""âœ… æˆåŠŸåŠ è½½ä¸­æ–‡PDFæ–‡ä»¶ï¼
ğŸ“Š æ–‡æ¡£ä¿¡æ¯ï¼š
  - æ–‡æ¡£é¡µæ•°: {len(documents)}
  - ç« èŠ‚æ•°: {len(splits)}ï¼ˆæŒ‰æ ‡é¢˜åˆ†å‰²ï¼‰
  - Embeddingæ¨¡å‹: BAAI/bge-base-zh-v1.5ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰
  - å‘é‡æ•°æ®åº“: FAISS
  
âœ¨ å·²å»ºç«‹å‘é‡ç´¢å¼•ï¼Œå¯ä»¥å¼€å§‹æŸ¥è¯¢åˆ†æè´¢åŠ¡æ•°æ®ï¼"""
            
        except Exception as emb_error:
            return f"""âŒ åˆ›å»ºå‘é‡ç´¢å¼•å¤±è´¥: {str(emb_error)}

ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š
1. è¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–ï¼špip install sentence-transformers
2. é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦400MBï¼‰ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸
3. å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•æ‰‹åŠ¨è®¾ç½®é•œåƒæºæˆ–ä½¿ç”¨ä»£ç†"""
    
    except Exception as e:
        return f"âŒ åŠ è½½PDFæ–‡ä»¶å¤±è´¥: {str(e)}\n\nğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿PDFæ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œä¸”æ–‡ä»¶æœªæŸåã€‚"






def get_vectorstore():
    """è·å–å½“å‰çš„å‘é‡å­˜å‚¨å®ä¾‹"""
    global pdf_vectorstore
    return pdf_vectorstore


def get_pdf_content():
    """è·å–å½“å‰çš„PDFåŸå§‹å†…å®¹"""
    global pdf_content
    return pdf_content


# å¯¼å‡ºæ‰€æœ‰å·¥å…·å’Œå‡½æ•°
__all__ = [
    'load_financial_pdf',
    'extract_financial_metrics',
    'extract_financial_table',
    'split_by_chinese_headers',
    'extract_number_from_text',
    'get_vectorstore',
    'get_pdf_content',
    'pdf_vectorstore',
    'pdf_content',
]

