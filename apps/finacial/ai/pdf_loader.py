"""
PDF æ•°æ®åŠ è½½å·¥å…·é›†
åŒ…å« PDF æ–‡ä»¶åŠ è½½ã€è§£æå’Œæ•°æ®æå–ç›¸å…³çš„å·¥å…·å‡½æ•°
"""

import os
import re
from langchain_core.tools import tool
from langchain_core.documents import Document
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings


# å…¨å±€å˜é‡ï¼šå­˜å‚¨åŠ è½½çš„PDFå‘é‡æ•°æ®åº“
pdf_vectorstore = None
pdf_content = None


def extract_financial_metrics(documents: list) -> dict:
    """
    ä» PDF æ–‡æ¡£ä¸­æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡
    
    Args:
        documents: PyMuPDFLoader åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
    
    Returns:
        åŒ…å«è´¢åŠ¡æŒ‡æ ‡çš„å­—å…¸
    """
    # åˆå¹¶æ‰€æœ‰é¡µé¢å†…å®¹
    full_text = "\n".join([doc.page_content for doc in documents])
    
    metrics = {}
    
    # å®šä¹‰è¦æå–çš„è´¢åŠ¡æŒ‡æ ‡åŠå…¶æ­£åˆ™æ¨¡å¼
    # æ ¼å¼ï¼šæŒ‡æ ‡åç§° -> (æ­£åˆ™æ¨¡å¼, æ•°å€¼ç»„ç´¢å¼•)
    patterns = {
        # åˆ©æ¶¦è¡¨æŒ‡æ ‡ (æ¯ä¸ªæ±‰å­—ä¹‹é—´éƒ½å…è®¸ç©ºç™½ç¬¦,æœ€å¤§åŒ–å…¼å®¹æ€§)
        "è¥ä¸šæ”¶å…¥": r"è¥[\s\n]*ä¸š[\s\n]*(?:æ€»[\s\n]*)?æ”¶[\s\n]*å…¥[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*([\d,ï¼Œ]+\.?\d*)",
        "åˆ©æ¶¦æ€»é¢": r"åˆ©[\s\n]*æ¶¦[\s\n]*æ€»[\s\n]*é¢[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*([\d,ï¼Œ]+\.?\d*)",
        "å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦": r"å½’[\s\n]*å±[\s\n]*äº[\s\n]*ä¸Š[\s\n]*å¸‚[\s\n]*å…¬[\s\n]*å¸[\s\n]*è‚¡[\s\n]*ä¸œ[\s\n]*çš„?[\s\n]*å‡€[\s\n]*åˆ©[\s\n]*æ¶¦[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d,ï¼Œ]+\.?\d*)",
        "æ‰£éå‡€åˆ©æ¶¦": r"æ‰£[\s\n]*é™¤[\s\n]*é?[\s\n]*ç»[\s\n]*å¸¸[\s\n]*æ€§[\s\n]*æŸ[\s\n]*ç›Š[\s\n]*çš„?[\s\n]*å‡€[\s\n]*åˆ©[\s\n]*æ¶¦[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d,ï¼Œ]+\.?\d*)",
        
        # æ¯è‚¡æŒ‡æ ‡
        "åŸºæœ¬æ¯è‚¡æ”¶ç›Š": r"åŸº[\s\n]*æœ¬[\s\n]*æ¯[\s\n]*è‚¡[\s\n]*æ”¶[\s\n]*ç›Š[ï¼ˆ(]?å…ƒ/è‚¡[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d.]+)",
        "ç¨€é‡Šæ¯è‚¡æ”¶ç›Š": r"ç¨€[\s\n]*é‡Š[\s\n]*æ¯[\s\n]*è‚¡[\s\n]*æ”¶[\s\n]*ç›Š[ï¼ˆ(]?å…ƒ/è‚¡[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d.]+)",
        
        # èµ„äº§è´Ÿå€ºè¡¨æŒ‡æ ‡
        "æ€»èµ„äº§": r"æ€»[\s\n]*èµ„[\s\n]*äº§[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*([\d,ï¼Œ]+\.?\d*)",
        "å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰€æœ‰è€…æƒç›Š": r"å½’[\s\n]*å±[\s\n]*äº[\s\n]*ä¸Š[\s\n]*å¸‚[\s\n]*å…¬[\s\n]*å¸[\s\n]*è‚¡[\s\n]*ä¸œ[\s\n]*çš„?[\s\n]*æ‰€[\s\n]*æœ‰[\s\n]*è€…[\s\n]*æƒ[\s\n]*ç›Š[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*([\d,ï¼Œ]+\.?\d*)",
        
        # ç°é‡‘æµæŒ‡æ ‡
        "ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢": r"ç»[\s\n]*è¥[\s\n]*æ´»[\s\n]*åŠ¨[\s\n]*äº§[\s\n]*ç”Ÿ[\s\n]*çš„[\s\n]*ç°[\s\n]*é‡‘[\s\n]*æµ[\s\n]*é‡[\s\n]*å‡€[\s\n]*é¢[ï¼ˆ(]?å…ƒ?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d,ï¼Œ]+\.?\d*)",
        
        # æ”¶ç›Šç‡æŒ‡æ ‡
        "åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡": r"åŠ [\s\n]*æƒ[\s\n]*å¹³[\s\n]*å‡[\s\n]*å‡€[\s\n]*èµ„[\s\n]*äº§[\s\n]*æ”¶[\s\n]*ç›Š[\s\n]*ç‡[ï¼ˆ(]?%?[)ï¼‰]?[\s\n|ï½œ]*(?:â€”[\s\n]*)*(-?[\d.]+)%?",
    }
    
    for metric_name, pattern in patterns.items():
        match = re.search(pattern, full_text)
        if match:
            value_str = match.group(1)
            # æ¸…ç†æ•°å€¼ï¼šç§»é™¤é€—å·ï¼Œè½¬æ¢ä¸ºæµ®ç‚¹æ•°
            value_str = value_str.replace(",", "").replace("ï¼Œ", "")
            try:
                metrics[metric_name] = float(value_str)
            except ValueError:
                metrics[metric_name] = value_str
    
    return metrics


def extract_financial_table(page_content: str) -> list:
    """
    ä»å•é¡µå†…å®¹ä¸­æå–è¡¨æ ¼æ•°æ®
    
    Args:
        page_content: å•é¡µçš„æ–‡æœ¬å†…å®¹
    
    Returns:
        è¡¨æ ¼è¡Œæ•°æ®åˆ—è¡¨
    """
    rows = []
    lines = page_content.split('\n')
    
    for line in lines:
        # åŒ¹é…åŒ…å«æ•°å­—çš„è¡Œï¼ˆå¯èƒ½æ˜¯è¡¨æ ¼æ•°æ®ï¼‰
        # è´¢åŠ¡æŠ¥è¡¨ä¸­çš„æ•°æ®è¡Œé€šå¸¸åŒ…å«å¤šä¸ªæ•°å­—
        numbers = re.findall(r'-?[\d,ï¼Œ]+\.?\d*', line)
        if len(numbers) >= 2:  # è‡³å°‘æœ‰ä¸¤ä¸ªæ•°å­—æ‰è®¤ä¸ºæ˜¯è¡¨æ ¼è¡Œ
            # æå–è¡Œé¦–çš„é¡¹ç›®åç§°
            item_name = re.sub(r'[\d,ï¼Œ.%\-\s]+', '', line).strip()
            if item_name:
                rows.append({
                    "item": item_name,
                    "values": [n.replace(",", "").replace("ï¼Œ", "") for n in numbers]
                })
    
    return rows


def split_by_chinese_headers(text: str, source: str = "") -> list:
    """
    æŒ‰ä¸­æ–‡è´¢åŠ¡æŠ¥è¡¨æ ‡é¢˜åˆ†å‰²æ–‡æœ¬
    
    æ”¯æŒçš„æ ‡é¢˜æ ¼å¼ï¼š
    - ä¸€ã€äºŒã€ä¸‰ã€... ï¼ˆä¸­æ–‡æ•°å­—åºå·ï¼‰
    - ç¬¬ä¸€èŠ‚ã€ç¬¬äºŒèŠ‚ã€... ï¼ˆç« èŠ‚æ ¼å¼ï¼‰
    - ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ï¼ˆä¸‰ï¼‰... ï¼ˆæ‹¬å·æ ¼å¼ï¼‰
    - 1ã€2ã€3ã€... æˆ– 1.  2.  3. ï¼ˆé˜¿æ‹‰ä¼¯æ•°å­—åºå·ï¼‰
    
    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        source: æ¥æºæ–‡ä»¶è·¯å¾„
    
    Returns:
        åˆ†å‰²åçš„ Document åˆ—è¡¨
    """
    # åŒ¹é…ä¸­æ–‡è´¢åŠ¡æŠ¥è¡¨å¸¸è§çš„æ ‡é¢˜æ ¼å¼
    header_pattern = re.compile(
        r'^('
        r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€|'                    # ä¸€ã€äºŒã€ä¸‰ã€
        r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[èŠ‚ç« æ¡æ¬¾]|'        # ç¬¬ä¸€èŠ‚ã€ç¬¬äºŒç« 
        r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[)ï¼‰]|'          # ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰(1)(2)
        r'\d+[ã€.ï¼]\s*[^\d]|'                          # 1ã€ 2. 3ï¼
        r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]'                           # åœˆæ•°å­—
        r')',
        re.MULTILINE
    )
    
    # æ‰¾åˆ°æ‰€æœ‰æ ‡é¢˜çš„ä½ç½®
    matches = list(header_pattern.finditer(text))
    
    if not matches:
        # æ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªæ–‡æ¡£
        return [Document(page_content=text.strip(), metadata={"source": source, "header": "å…¨æ–‡"})]
    
    documents = []
    
    for i, match in enumerate(matches):
        start = match.start()
        # ä¸‹ä¸€ä¸ªæ ‡é¢˜çš„èµ·å§‹ä½ç½®ï¼Œæˆ–è€…æ–‡æœ¬æœ«å°¾
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        
        chunk_text = text[start:end].strip()
        
        if chunk_text:
            # æå–æ ‡é¢˜ä½œä¸ºå…ƒæ•°æ®
            header_text = match.group(1).strip()
            # è·å–æ ‡é¢˜åçš„ç¬¬ä¸€è¡Œä½œä¸ºå®Œæ•´æ ‡é¢˜
            first_line = chunk_text.split('\n')[0][:50]  # é™åˆ¶é•¿åº¦
            
            documents.append(Document(
                page_content=chunk_text,
                metadata={
                    "source": source,
                    "header": first_line,
                    "header_marker": header_text
                }
            ))
    
    # å¦‚æœç¬¬ä¸€ä¸ªæ ‡é¢˜ä¹‹å‰æœ‰å†…å®¹ï¼Œä¹Ÿä½œä¸ºä¸€ä¸ªæ–‡æ¡£
    if matches and matches[0].start() > 0:
        pre_content = text[:matches[0].start()].strip()
        if pre_content and len(pre_content) > 50:  # å¿½ç•¥å¤ªçŸ­çš„å‰ç½®å†…å®¹
            documents.insert(0, Document(
                page_content=pre_content,
                metadata={"source": source, "header": "æ–‡æ¡£å¼€å¤´"}
            ))
    
    return documents


def expand_query_with_synonyms(query: str, max_expansion: int = 3) -> list:
    """
    æ‰©å±•æŸ¥è¯¢è¯ï¼Œå¢åŠ è´¢åŠ¡é¢†åŸŸåŒä¹‰è¯/ç›¸å…³è¯ï¼ˆç²¾ç®€ç‰ˆï¼‰
    
    Args:
        query: åŸå§‹æŸ¥è¯¢è¯
        max_expansion: æœ€å¤§æ‰©å±•è¯æ•°é‡ï¼ˆé»˜è®¤3ä¸ªï¼Œé¿å…æœç´¢è¿‡å¤šï¼‰
    
    Returns:
        æ‰©å±•åçš„æŸ¥è¯¢è¯åˆ—è¡¨
    """
    # è´¢åŠ¡é¢†åŸŸåŒä¹‰è¯æ˜ å°„ï¼ˆç²¾ç®€ç‰ˆï¼Œåªä¿ç•™æœ€å¸¸ç”¨çš„å˜ä½“ï¼‰
    financial_synonyms = {
        "åˆ©æ¶¦": ["åˆ©æ¶¦æ€»é¢", "å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦", "å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦"],
        "æ”¶å…¥": ["è¥ä¸šæ”¶å…¥", "è¥ä¸šæ€»æ”¶å…¥"],
        "èµ„äº§": ["æ€»èµ„äº§", "èµ„äº§æ€»è®¡"],
        "è´Ÿå€º": ["æ€»è´Ÿå€º", "è´Ÿå€ºåˆè®¡"],
        "ç°é‡‘æµ": ["ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢"],
        "æ¯›åˆ©": ["æ¯›åˆ©ç‡"],
        "å‡€åˆ©ç‡": ["é”€å”®å‡€åˆ©ç‡"],
        "ROE": ["å‡€èµ„äº§æ”¶ç›Šç‡"],
        "ROA": ["æ€»èµ„äº§æ”¶ç›Šç‡"],
        "EPS": ["åŸºæœ¬æ¯è‚¡æ”¶ç›Šï¼ˆå…ƒ/è‚¡ï¼‰", "ç¨€é‡Šæ¯è‚¡æ”¶ç›Šï¼ˆå…ƒ/è‚¡ï¼‰"],
        "è¥æ”¶": ["è¥ä¸šæ”¶å…¥"],
        "æˆæœ¬": ["è¥ä¸šæˆæœ¬"],
        "è´¹ç”¨": ["é”€å”®è´¹ç”¨", "ç®¡ç†è´¹ç”¨", "è´¢åŠ¡è´¹ç”¨"],
    }
    
    queries = [query]
    
    # åªåŒ¹é…ç¬¬ä¸€ä¸ªå‘½ä¸­çš„å…³é”®è¯ï¼Œé¿å…è¿‡åº¦æ‰©å±•
    for key, synonyms in financial_synonyms.items():
        if key in query:
            # åªæ·»åŠ æœ‰é™æ•°é‡çš„åŒä¹‰è¯
            queries.extend(synonyms[:max_expansion])
            break
        for syn in synonyms:
            if syn in query:
                queries.append(key)
                break
    
    # å»é‡å¹¶è¿”å›ï¼Œé™åˆ¶æ€»æ•°é‡
    unique_queries = list(set(queries))
    return unique_queries[:max_expansion + 1]  # åŸå§‹æŸ¥è¯¢ + max_expansion ä¸ªæ‰©å±•è¯


def extract_number_from_text(text: str) -> list:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰æ•°å­—ï¼ˆæ”¯æŒåƒåˆ†ä½ã€å°æ•°ã€è´Ÿæ•°ã€å¸¦å•ä½ï¼‰
    
    Returns:
        [(æ•°å­—å€¼, åŸå§‹å­—ç¬¦ä¸²), ...]
    """
    # åŒ¹é…å„ç§æ•°å­—æ ¼å¼ï¼šè´Ÿæ•°ã€åƒåˆ†ä½ã€å°æ•°ã€å¸¦å•ä½ï¼ˆä¸‡ã€äº¿ã€å…ƒï¼‰
    patterns = [
        r'(-?[\d,ï¼Œ]+\.?\d*)\s*(?:ä¸‡å…ƒ|äº¿å…ƒ|å…ƒ)?',
        r'(-?[\d,ï¼Œ]+\.?\d*)',
    ]
    
    results = []
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                match = match[0]
            # æ¸…ç†æ ¼å¼
            clean_num = match.replace(',', '').replace('ï¼Œ', '').strip()
            if clean_num and clean_num not in ['-', '.']:
                try:
                    value = float(clean_num)
                    if abs(value) > 0:  # æ’é™¤0
                        results.append((value, match))
                except:
                    pass
    
    # å»é‡
    seen = set()
    unique_results = []
    for value, original in results:
        if value not in seen:
            seen.add(value)
            unique_results.append((value, original))
    
    return unique_results


@tool
def load_financial_pdf(pdf_path: str) -> str:
    """
    åŠ è½½å¹¶å¤„ç†è´¢åŠ¡æŠ¥è¡¨PDFæ–‡ä»¶ï¼ˆä¸­æ–‡ä¼˜åŒ–ç‰ˆï¼‰
    
    Args:
        pdf_path: PDFæ–‡ä»¶çš„è·¯å¾„
    
    Returns:
        åŠ è½½çŠ¶æ€ä¿¡æ¯
    """
    global pdf_vectorstore, pdf_content
    
    try:
        # ä½¿ç”¨ PyMuPDF åŠ è½½PDFï¼ˆå¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½ï¼‰
        print("ğŸ“‚ æ­£åœ¨åŠ è½½PDFæ–‡ä»¶...")
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        print(documents[0].page_content)
        print(f"âœ“ å·²åŠ è½½ {len(documents)} é¡µ")
        
        # ä¿å­˜åŸå§‹å†…å®¹
        pdf_content = "\n\n".join([doc.page_content for doc in documents])
        
        # æŒ‰æ ‡é¢˜åˆ†å‰²ï¼ˆä¿æŒè´¢åŠ¡æŠ¥è¡¨ç« èŠ‚å®Œæ•´æ€§ï¼‰
        print("ğŸ“ æ­£åœ¨æŒ‰æ ‡é¢˜åˆ†å‰²æ–‡æœ¬...")
        source = documents[0].metadata.get("source", pdf_path) if documents else pdf_path
        splits = split_by_chinese_headers(pdf_content, source)
        print(f"âœ“ æŒ‰æ ‡é¢˜åˆ†å‰²ï¼Œå…± {len(splits)} ä¸ªç« èŠ‚")
        
        # ä½¿ç”¨æœ¬åœ°ä¸­æ–‡ Embedding æ¨¡å‹åˆ›å»ºå‘é‡å­˜å‚¨
        try:
            print("ğŸ”§ æ­£åœ¨åŠ è½½ä¸­æ–‡ Embedding æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰...")
            embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-base-zh-v1.5",  # ä¸“é—¨çš„ä¸­æ–‡ Embedding æ¨¡å‹ï¼Œçº¦400MB
                model_kwargs={'device': 'cpu'},  # ä½¿ç”¨ CPUï¼Œå¦‚æœ‰ GPU å¯æ”¹ä¸º 'cuda'
                encode_kwargs={'normalize_embeddings': True}
            )
            
            print("ğŸ” æ­£åœ¨åˆ›å»ºå‘é‡ç´¢å¼•...")
            pdf_vectorstore = FAISS.from_documents(splits, embeddings)
            print("âœ“ å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆ")
            
            return f"""âœ… æˆåŠŸåŠ è½½ä¸­æ–‡PDFæ–‡ä»¶ï¼
ğŸ“Š æ–‡æ¡£ä¿¡æ¯ï¼š
  - æ–‡æ¡£é¡µæ•°: {len(documents)}
  - ç« èŠ‚æ•°: {len(splits)}ï¼ˆæŒ‰æ ‡é¢˜åˆ†å‰²ï¼‰
  - Embeddingæ¨¡å‹: BAAI/bge-base-zh-v1.5ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰
  - å‘é‡æ•°æ®åº“: FAISS
  
âœ¨ å·²å»ºç«‹å‘é‡ç´¢å¼•ï¼Œå¯ä»¥å¼€å§‹æŸ¥è¯¢åˆ†æè´¢åŠ¡æ•°æ®ï¼"""
            
        except Exception as emb_error:
            return f"""âŒ åˆ›å»ºå‘é‡ç´¢å¼•å¤±è´¥: {str(emb_error)}

ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š
1. è¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–ï¼špip install sentence-transformers
2. é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦400MBï¼‰ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸
3. å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•æ‰‹åŠ¨è®¾ç½®é•œåƒæºæˆ–ä½¿ç”¨ä»£ç†"""
    
    except Exception as e:
        return f"âŒ åŠ è½½PDFæ–‡ä»¶å¤±è´¥: {str(e)}\n\nğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿PDFæ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œä¸”æ–‡ä»¶æœªæŸåã€‚"


@tool
def search_financial_info(query: str) -> str:
    """
    ä»å·²åŠ è½½çš„è´¢åŠ¡æŠ¥è¡¨PDFä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
    
    Args:
        query: è¦æŸ¥è¯¢çš„è´¢åŠ¡ä¿¡æ¯ï¼ˆå¦‚"è¥ä¸šæ”¶å…¥"ã€"å‡€åˆ©æ¶¦"ã€"èµ„äº§è´Ÿå€ºè¡¨"ã€"å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦"ç­‰ï¼‰
    
    Returns:
        æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯
    """
    global pdf_vectorstore
    
    if pdf_vectorstore is None:
        return "âŒ è¯·å…ˆä½¿ç”¨ load_financial_pdf å·¥å…·åŠ è½½PDFæ–‡ä»¶"
    
    try:
        # æ‰©å±•æŸ¥è¯¢è¯
        expanded_queries = expand_query_with_synonyms(query)
        
        all_docs = []
        seen_contents = set()
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¯è¿›è¡Œæ£€ç´¢
        for q in expanded_queries:
            # ä½¿ç”¨å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢ï¼Œè·å–æ›´å¤šå€™é€‰
            docs_with_scores = pdf_vectorstore.similarity_search_with_score(q, k=5)
            
            for doc, score in docs_with_scores:
                # è¿‡æ»¤ä½ç›¸å…³åº¦ç»“æœï¼ˆåˆ†æ•°è¶Šä½è¶Šç›¸ä¼¼ï¼ŒFAISS ä½¿ç”¨ L2 è·ç¦»ï¼‰
                # åŒæ—¶å»é‡
                content_hash = hash(doc.page_content[:100])
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    all_docs.append((doc, score, q))
        
        if not all_docs:
            return f"æœªæ‰¾åˆ°å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯"
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åºï¼ˆåˆ†æ•°è¶Šä½è¶Šå¥½ï¼‰
        all_docs.sort(key=lambda x: x[1])
        
        # å–å‰5ä¸ªæœ€ç›¸å…³çš„ç»“æœ
        top_docs = all_docs[:5]
        
        # æ•´åˆæ£€ç´¢ç»“æœ
        result = f"ğŸ“„ å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯ï¼š\n\n"
        for i, (doc, score, matched_query) in enumerate(top_docs, 1):
            result += f"ç‰‡æ®µ {i} (åŒ¹é…è¯: {matched_query}, ç›¸å…³åº¦: {1/(1+score):.2%}):\n{doc.page_content}\n\n{'='*50}\n\n"
        
        return result
    
    except Exception as e:
        return f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}"


@tool
def extract_financial_data(data_type: str) -> str:
    """
    ä»PDFä¸­æå–ç‰¹å®šçš„è´¢åŠ¡æ•°æ®ï¼ˆé€šè¿‡ RAG æ£€ç´¢ + æ™ºèƒ½åŒ¹é…ï¼‰
    
    Args:
        data_type: æ•°æ®ç±»å‹ï¼Œå¯é€‰å€¼åŒ…æ‹¬ï¼š
            - 'revenue': è¥ä¸šæ”¶å…¥
            - 'net_income': å‡€åˆ©æ¶¦  
            - 'total_assets': æ€»èµ„äº§
            - 'total_liabilities': æ€»è´Ÿå€º
            - 'equity': è‚¡ä¸œæƒç›Š
            - 'current_assets': æµåŠ¨èµ„äº§
            - 'current_liabilities': æµåŠ¨è´Ÿå€º
            - 'cash': ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©
            - 'operating_income': å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦
            - 'all': æå–æ‰€æœ‰å…³é”®è´¢åŠ¡æŒ‡æ ‡
    
    Returns:
        æå–çš„è´¢åŠ¡æ•°æ®åŠç›¸å…³ä¸Šä¸‹æ–‡
    """
    global pdf_vectorstore, pdf_content
    
    if pdf_vectorstore is None:
        return "âŒ è¯·å…ˆä½¿ç”¨ load_financial_pdf å·¥å…·åŠ è½½PDFæ–‡ä»¶"
    
    # è´¢åŠ¡æŒ‡æ ‡çš„æ£€ç´¢å…³é”®è¯å’Œåˆ«å
    data_config = {
        'revenue': {
            'name': 'è¥ä¸šæ”¶å…¥',
            'keywords': ['è¥ä¸šæ”¶å…¥', 'è¥ä¸šæ€»æ”¶å…¥', 'ä¸»è¥ä¸šåŠ¡æ”¶å…¥', 'ä¸€ã€è¥ä¸šæ”¶å…¥'],
            'patterns': [
                r'(?:ä¸€ã€)?è¥ä¸š(?:æ€»)?æ”¶å…¥[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
                r'è¥ä¸šæ”¶å…¥\s+([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'net_income': {
            'name': 'å‡€åˆ©æ¶¦',
            'keywords': ['å‡€åˆ©æ¶¦', 'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦', 'å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦'],
            'patterns': [
                r'(?:å››ã€)?å‡€åˆ©æ¶¦[^\d]*?([\d,ï¼Œ-]+(?:\.\d+)?)',
                r'å½’å±äº.*?å‡€åˆ©æ¶¦[^\d]*?([\d,ï¼Œ-]+(?:\.\d+)?)',
            ]
        },
        'total_assets': {
            'name': 'æ€»èµ„äº§',
            'keywords': ['èµ„äº§æ€»è®¡', 'èµ„äº§æ€»é¢', 'æ€»èµ„äº§'],
            'patterns': [
                r'èµ„äº§æ€»è®¡[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
                r'æ€»èµ„äº§[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'total_liabilities': {
            'name': 'æ€»è´Ÿå€º',
            'keywords': ['è´Ÿå€ºåˆè®¡', 'è´Ÿå€ºæ€»è®¡', 'è´Ÿå€ºæ€»é¢'],
            'patterns': [
                r'è´Ÿå€º(?:åˆè®¡|æ€»è®¡)[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'equity': {
            'name': 'è‚¡ä¸œæƒç›Š',
            'keywords': ['æ‰€æœ‰è€…æƒç›Šåˆè®¡', 'è‚¡ä¸œæƒç›Šåˆè®¡', 'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…æƒç›Š'],
            'patterns': [
                r'(?:æ‰€æœ‰è€…|è‚¡ä¸œ)æƒç›Š.*?åˆè®¡[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
                r'å½’å±äºæ¯å…¬å¸.*?æƒç›Š[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'current_assets': {
            'name': 'æµåŠ¨èµ„äº§',
            'keywords': ['æµåŠ¨èµ„äº§åˆè®¡', 'æµåŠ¨èµ„äº§å°è®¡'],
            'patterns': [
                r'æµåŠ¨èµ„äº§(?:åˆè®¡|å°è®¡)[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'current_liabilities': {
            'name': 'æµåŠ¨è´Ÿå€º',
            'keywords': ['æµåŠ¨è´Ÿå€ºåˆè®¡', 'æµåŠ¨è´Ÿå€ºå°è®¡'],
            'patterns': [
                r'æµåŠ¨è´Ÿå€º(?:åˆè®¡|å°è®¡)[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'cash': {
            'name': 'è´§å¸èµ„é‡‘',
            'keywords': ['è´§å¸èµ„é‡‘', 'ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©', 'åº“å­˜ç°é‡‘'],
            'patterns': [
                r'è´§å¸èµ„é‡‘[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
                r'ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'operating_income': {
            'name': 'å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦',
            'keywords': ['æ‰£é™¤éç»å¸¸æ€§æŸç›Š', 'æ‰£éå‡€åˆ©æ¶¦', 'æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦'],
            'patterns': [
                r'æ‰£é™¤éç»å¸¸æ€§æŸç›Š.*?å‡€åˆ©æ¶¦[^\d]*?([\d,ï¼Œ-]+(?:\.\d+)?)',
                r'å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦[^\d]*?([\d,ï¼Œ-]+(?:\.\d+)?)',
            ]
        },
    }
    
    def search_and_extract(config):
        """ä½¿ç”¨ RAG æ£€ç´¢å¹¶æå–æ•°æ®"""
        all_context = []
        extracted_values = []
        
        # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæ£€ç´¢
        for keyword in config['keywords']:
            try:
                docs = pdf_vectorstore.similarity_search(keyword, k=3)
                for doc in docs:
                    content = doc.page_content
                    all_context.append(content)
                    
                    # å°è¯•ç”¨æ­£åˆ™æå–æ•°å€¼
                    for pattern in config['patterns']:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        for match in matches:
                            clean_num = match.replace(',', '').replace('ï¼Œ', '').strip()
                            try:
                                value = float(clean_num)
                                if abs(value) > 0:
                                    extracted_values.append(value)
                            except:
                                pass
            except:
                pass
        
        # è¿”å›æ‰¾åˆ°çš„å€¼ï¼ˆå–æœ€å¤§å€¼ï¼Œé€šå¸¸è´¢åŠ¡æŠ¥è¡¨çš„åˆè®¡æ•°è¾ƒå¤§ï¼‰
        unique_values = list(set(extracted_values))
        unique_values.sort(reverse=True)
        
        return unique_values, all_context
    
    if data_type == 'all':
        # æå–æ‰€æœ‰æŒ‡æ ‡
        result = "ğŸ“Š æå–çš„è´¢åŠ¡æ•°æ®ï¼š\n\n"
        extracted_data = {}
        
        for key, config in data_config.items():
            values, contexts = search_and_extract(config)
            if values:
                # å–æœ€å¯èƒ½çš„å€¼ï¼ˆé€šå¸¸æ˜¯æœ€å¤§çš„ï¼‰
                value = values[0]
                extracted_data[key] = value
                result += f"- {config['name']}: {value:,.2f}\n"
            else:
                result += f"- {config['name']}: æœªæ‰¾åˆ°\n"
        
        # å¦‚æœæå–åˆ°æ•°æ®è¾ƒå°‘ï¼Œé™„åŠ åŸå§‹ä¸Šä¸‹æ–‡ä¾› LLM åˆ†æ
        found_count = len([v for v in extracted_data.values() if v])
        if found_count < 5:
            result += "\n\nâš ï¸ éƒ¨åˆ†æ•°æ®æœªèƒ½è‡ªåŠ¨æå–ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³åŸå§‹å†…å®¹ä¾›åˆ†æï¼š\n\n"
            # æ£€ç´¢ä¸»è¦è´¢åŠ¡æŠ¥è¡¨åŒºåŸŸ
            for keyword in ['åˆ©æ¶¦è¡¨', 'èµ„äº§è´Ÿå€ºè¡¨', 'ä¸»è¦ä¼šè®¡æ•°æ®']:
                try:
                    docs = pdf_vectorstore.similarity_search(keyword, k=2)
                    for doc in docs:
                        result += f"---\n{doc.page_content[:500]}\n"
                except:
                    pass
        
        return result
    
    elif data_type in data_config:
        config = data_config[data_type]
        values, contexts = search_and_extract(config)
        
        if values:
            result = f"ğŸ“Š {config['name']}: {values[0]:,.2f}\n"
            if len(values) > 1:
                result += f"   (å…¶ä»–å€™é€‰å€¼: {', '.join([f'{v:,.2f}' for v in values[1:3]])})\n"
            result += f"\nç›¸å…³ä¸Šä¸‹æ–‡:\n{contexts[0][:300] if contexts else 'æ— '}..."
            return result
        else:
            # è¿”å›æ£€ç´¢åˆ°çš„åŸå§‹å†…å®¹ï¼Œè®© LLM è‡ªè¡Œåˆ†æ
            result = f"â“ æœªèƒ½è‡ªåŠ¨æå– {config['name']}ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³å†…å®¹ï¼š\n\n"
            for keyword in config['keywords'][:2]:
                try:
                    docs = pdf_vectorstore.similarity_search(keyword, k=2)
                    for doc in docs:
                        result += f"---\n{doc.page_content[:400]}\n"
                except:
                    pass
            return result
    
    else:
        return f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}"


def get_vectorstore():
    """è·å–å½“å‰çš„å‘é‡å­˜å‚¨å®ä¾‹"""
    global pdf_vectorstore
    return pdf_vectorstore


def get_pdf_content():
    """è·å–å½“å‰çš„PDFåŸå§‹å†…å®¹"""
    global pdf_content
    return pdf_content


# å¯¼å‡ºæ‰€æœ‰å·¥å…·å’Œå‡½æ•°
__all__ = [
    'load_financial_pdf',
    'search_financial_info',
    'extract_financial_data',
    'extract_financial_metrics',
    'extract_financial_table',
    'split_by_chinese_headers',
    'expand_query_with_synonyms',
    'extract_number_from_text',
    'get_vectorstore',
    'get_pdf_content',
    'pdf_vectorstore',
    'pdf_content',
]

