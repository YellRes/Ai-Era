"""
è´¢åŠ¡æŠ¥è¡¨åˆ†ææ™ºèƒ½ä½“
åŸºäº LangChain å’Œ DeepSeek åˆ›å»ºçš„æ™ºèƒ½è´¢åŠ¡åˆ†æåŠ©æ‰‹
"""

import os
import re
import sys
import logging
from pathlib import Path
from typing import Optional, Generator
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# ç¦ç”¨ httpx çš„ HTTP è¯·æ±‚æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

from langchain_core.tools import tool
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# PDF ä¿å­˜ç›®å½•ï¼ˆç›¸å¯¹äºè„šæœ¬ä½ç½®çš„ä¸Šçº§ç›®å½•ä¸­çš„ pdf æ–‡ä»¶å¤¹ï¼‰
PDF_DIR = os.path.join(SCRIPT_DIR, '..', 'pdf')


# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸º UTF-8ï¼ˆä¿®å¤ Windows ä¸‹çš„ç¼–ç é—®é¢˜ï¼‰
if sys.platform.startswith('win'):
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except:
        pass

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ£€æŸ¥ API å¯†é’¥
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
if not DEEPSEEK_API_KEY:
    print("âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
    print("è¯·åœ¨é¡¹ç›®ç›®å½•ä¸‹åˆ›å»º .env æ–‡ä»¶å¹¶æ·»åŠ ï¼š")
    print("DEEPSEEK_API_KEY=your_api_key_here\n")

# å…¨å±€å˜é‡ï¼šå­˜å‚¨åŠ è½½çš„PDFå‘é‡æ•°æ®åº“
pdf_vectorstore = None
pdf_content = None


# å®šä¹‰è´¢åŠ¡åˆ†æå·¥å…·
@tool
def calculate_financial_ratio(metric: str, numerator: float, denominator: float) -> str:
    """
    è®¡ç®—è´¢åŠ¡æ¯”ç‡
    
    Args:
        metric: æ¯”ç‡åç§°ï¼ˆå¦‚ 'ROE', 'ROA', 'current_ratio', 'debt_ratio'ï¼‰
        numerator: åˆ†å­
        denominator: åˆ†æ¯
    
    Returns:
        è®¡ç®—ç»“æœçš„æè¿°
    """
    if denominator == 0:
        return f"é”™è¯¯ï¼šåˆ†æ¯ä¸èƒ½ä¸ºé›¶"
    
    ratio = numerator / denominator
    
    metric_names = {
        'ROE': 'å‡€èµ„äº§æ”¶ç›Šç‡',
        'ROA': 'æ€»èµ„äº§æ”¶ç›Šç‡',
        'current_ratio': 'æµåŠ¨æ¯”ç‡',
        'debt_ratio': 'èµ„äº§è´Ÿå€ºç‡',
        'profit_margin': 'åˆ©æ¶¦ç‡'
    }
    
    metric_name = metric_names.get(metric, metric)
    
    if metric in ['ROE', 'ROA', 'profit_margin', 'debt_ratio']:
        percentage = ratio * 100
        return f"{metric_name}: {percentage:.2f}%"
    else:
        return f"{metric_name}: {ratio:.2f}"


@tool
def analyze_profitability(revenue: float, net_income: float, total_assets: float, operating_income: float) -> str:
    """
    åˆ†æä¼ä¸šç›ˆåˆ©èƒ½åŠ›
    
    Args:
        revenue: è¥ä¸šæ”¶å…¥
        net_income: å‡€åˆ©æ¶¦
        total_assets: æ€»èµ„äº§
        operating_income: å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦
    
    Returns:
        ç›ˆåˆ©èƒ½åŠ›åˆ†ææŠ¥å‘Š
    """
    if revenue == 0 or total_assets == 0:
        return "é”™è¯¯ï¼šæ”¶å…¥æˆ–æ€»èµ„äº§ä¸èƒ½ä¸ºé›¶"
    
    profit_margin = (net_income / revenue) * 100
    roa = (net_income / total_assets) * 100
    operating_profit_margin = (operating_income / revenue) * 100
    analysis = f"""
ğŸ“Š ç›ˆåˆ©èƒ½åŠ›åˆ†ææŠ¥å‘Šï¼š
- åˆ©æ¶¦ç‡: {profit_margin:.2f}%
- æ€»èµ„äº§æ”¶ç›Šç‡(ROA): {roa:.2f}%
- å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦ç‡: {operating_profit_margin:.2f}%

ğŸ’¡ åˆ†æç»“è®ºï¼š
"""
    
    if profit_margin > 15:
        analysis += "- åˆ©æ¶¦ç‡è¡¨ç°ä¼˜ç§€ï¼Œç›ˆåˆ©èƒ½åŠ›å¼º\n"
    elif profit_margin > 5:
        analysis += "- åˆ©æ¶¦ç‡å¤„äºåˆç†æ°´å¹³\n"
    else:
        analysis += "- åˆ©æ¶¦ç‡åä½ï¼Œéœ€è¦å…³æ³¨æˆæœ¬æ§åˆ¶\n"
    
    if roa > 10:
        analysis += "- èµ„äº§ä½¿ç”¨æ•ˆç‡é«˜ï¼ŒæŠ•èµ„å›æŠ¥è‰¯å¥½\n"
    elif roa > 5:
        analysis += "- èµ„äº§ä½¿ç”¨æ•ˆç‡ä¸­ç­‰\n"
    else:
        analysis += "- èµ„äº§ä½¿ç”¨æ•ˆç‡è¾ƒä½ï¼Œéœ€è¦ä¼˜åŒ–èµ„äº§é…ç½®\n"
    
    return analysis + "å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦ç‡: {operating_profit_margin:.2f}%"


@tool
def analyze_liquidity(current_assets: float, current_liabilities: float, 
                      cash: float, inventory: float) -> str:
    """
    åˆ†æä¼ä¸šæµåŠ¨æ€§å’Œå¿å€ºèƒ½åŠ›
    
    Args:
        current_assets: æµåŠ¨èµ„äº§
        current_liabilities: æµåŠ¨è´Ÿå€º
        cash: ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©
        inventory: å­˜è´§
    
    Returns:
        æµåŠ¨æ€§åˆ†ææŠ¥å‘Š
    """
    if current_liabilities == 0:
        return "é”™è¯¯ï¼šæµåŠ¨è´Ÿå€ºä¸èƒ½ä¸ºé›¶"
    
    current_ratio = current_assets / current_liabilities
    quick_ratio = (current_assets - inventory) / current_liabilities
    cash_ratio = cash / current_liabilities
    
    analysis = f"""
ğŸ’° æµåŠ¨æ€§åˆ†ææŠ¥å‘Šï¼š
- æµåŠ¨æ¯”ç‡: {current_ratio:.2f}
- é€ŸåŠ¨æ¯”ç‡: {quick_ratio:.2f}
- ç°é‡‘æ¯”ç‡: {cash_ratio:.2f}

ğŸ’¡ åˆ†æç»“è®ºï¼š
"""
    
    if current_ratio >= 2:
        analysis += "- æµåŠ¨æ¯”ç‡å¥åº·ï¼ŒçŸ­æœŸå¿å€ºèƒ½åŠ›å¼º\n"
    elif current_ratio >= 1:
        analysis += "- æµåŠ¨æ¯”ç‡åŸºæœ¬åˆç†\n"
    else:
        analysis += "- æµåŠ¨æ¯”ç‡åä½ï¼Œå­˜åœ¨çŸ­æœŸå¿å€ºé£é™©\n"
    
    if quick_ratio >= 1:
        analysis += "- é€ŸåŠ¨æ¯”ç‡è‰¯å¥½ï¼Œå˜ç°èƒ½åŠ›å¼º\n"
    else:
        analysis += "- é€ŸåŠ¨æ¯”ç‡åä½ï¼Œéœ€è¦å…³æ³¨å­˜è´§å‘¨è½¬\n"
    
    return analysis


@tool
def analyze_leverage(total_assets: float, total_liabilities: float, 
                     equity: float, interest_expense: float, ebit: float) -> str:
    """
    åˆ†æä¼ä¸šæ æ†å’Œèµ„æœ¬ç»“æ„
    
    Args:
        total_assets: æ€»èµ„äº§
        total_liabilities: æ€»è´Ÿå€º
        equity: è‚¡ä¸œæƒç›Š
        interest_expense: åˆ©æ¯è´¹ç”¨
        ebit: æ¯ç¨å‰åˆ©æ¶¦
    
    Returns:
        æ æ†åˆ†ææŠ¥å‘Š
    """
    if total_assets == 0 or equity == 0:
        return "é”™è¯¯ï¼šæ€»èµ„äº§æˆ–è‚¡ä¸œæƒç›Šä¸èƒ½ä¸ºé›¶"
    
    debt_ratio = (total_liabilities / total_assets) * 100
    equity_ratio = (equity / total_assets) * 100
    debt_to_equity = total_liabilities / equity if equity != 0 else 0
    
    analysis = f"""
ğŸ¦ æ æ†ä¸èµ„æœ¬ç»“æ„åˆ†æï¼š
- èµ„äº§è´Ÿå€ºç‡: {debt_ratio:.2f}%
- è‚¡ä¸œæƒç›Šæ¯”ç‡: {equity_ratio:.2f}%
- è´Ÿå€ºæƒç›Šæ¯”: {debt_to_equity:.2f}

ğŸ’¡ åˆ†æç»“è®ºï¼š
"""
    
    if debt_ratio < 40:
        analysis += "- è´Ÿå€ºæ°´å¹³è¾ƒä½ï¼Œè´¢åŠ¡é£é™©å°\n"
    elif debt_ratio < 60:
        analysis += "- è´Ÿå€ºæ°´å¹³é€‚ä¸­ï¼Œèµ„æœ¬ç»“æ„åˆç†\n"
    else:
        analysis += "- è´Ÿå€ºæ°´å¹³è¾ƒé«˜ï¼Œéœ€è¦å…³æ³¨è´¢åŠ¡é£é™©\n"
    
    if interest_expense > 0 and ebit > 0:
        interest_coverage = ebit / interest_expense
        analysis += f"- åˆ©æ¯ä¿éšœå€æ•°: {interest_coverage:.2f}å€\n"
        if interest_coverage > 5:
            analysis += "  â†’ åˆ©æ¯å¿ä»˜èƒ½åŠ›å¼º\n"
        elif interest_coverage > 2:
            analysis += "  â†’ åˆ©æ¯å¿ä»˜èƒ½åŠ›å°šå¯\n"
        else:
            analysis += "  â†’ åˆ©æ¯å¿ä»˜å‹åŠ›è¾ƒå¤§\n"
    
    return analysis


@tool
def load_financial_pdf(pdf_path: str) -> str:
    """
    åŠ è½½å¹¶å¤„ç†è´¢åŠ¡æŠ¥è¡¨PDFæ–‡ä»¶ï¼ˆä¸­æ–‡ä¼˜åŒ–ç‰ˆï¼‰
    
    Args:
        pdf_path: PDFæ–‡ä»¶çš„è·¯å¾„
    
    Returns:
        åŠ è½½çŠ¶æ€ä¿¡æ¯
    """
    global pdf_vectorstore, pdf_content
    
    try:
        # ä½¿ç”¨ PyMuPDF åŠ è½½PDFï¼ˆå¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½ï¼‰
        print("ğŸ“‚ æ­£åœ¨åŠ è½½PDFæ–‡ä»¶...")
        # load_fn = PyMuPDFLoader if is_online else OnlinePDFLoader
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        print(f"âœ“ å·²åŠ è½½ {len(documents)} é¡µ")
        
        # ä¿å­˜åŸå§‹å†…å®¹
        pdf_content = "\n\n".join([doc.page_content for doc in documents])
        
        # ä¸­æ–‡ä¼˜åŒ–çš„æ–‡æœ¬åˆ†å‰²
        print("ğŸ“ æ­£åœ¨åˆ†å‰²æ–‡æœ¬...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # é€‚å½“å¢å¤§ï¼Œä¿è¯è´¢åŠ¡è¡¨æ ¼å®Œæ•´æ€§
            chunk_overlap=200,  # å¢åŠ é‡å ï¼Œé¿å…å…³é”®ä¿¡æ¯è¢«åˆ‡æ–­
            separators=[
                "\n\n",    # æ®µè½
                "\n",      # æ¢è¡Œ
                "ã€‚",      # ä¸­æ–‡å¥å·
                "ï¼",      # ä¸­æ–‡æ„Ÿå¹å·
                "ï¼Ÿ",      # ä¸­æ–‡é—®å·
                "ï¼›",      # ä¸­æ–‡åˆ†å·ï¼ˆè´¢åŠ¡æŠ¥è¡¨å¸¸ç”¨ï¼‰
                "ï¼Œ",      # ä¸­æ–‡é€—å·
                ".",       # è‹±æ–‡å¥å·
                "!",       # è‹±æ–‡æ„Ÿå¹å·
                "?",       # è‹±æ–‡é—®å·
                " ",       # ç©ºæ ¼
                ""         # å­—ç¬¦çº§åˆ«
            ],
            length_function=len,
        )
        splits = text_splitter.split_documents(documents)
        print(f"âœ“ å·²åˆ†å‰²ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")
        
        # ä½¿ç”¨æœ¬åœ°ä¸­æ–‡ Embedding æ¨¡å‹åˆ›å»ºå‘é‡å­˜å‚¨
        try:
            print("ğŸ”§ æ­£åœ¨åŠ è½½ä¸­æ–‡ Embedding æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰...")
            embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-base-zh-v1.5",  # ä¸“é—¨çš„ä¸­æ–‡ Embedding æ¨¡å‹ï¼Œçº¦400MB
                model_kwargs={'device': 'cpu'},  # ä½¿ç”¨ CPUï¼Œå¦‚æœ‰ GPU å¯æ”¹ä¸º 'cuda'
                encode_kwargs={'normalize_embeddings': True}
            )
            
            print("ğŸ” æ­£åœ¨åˆ›å»ºå‘é‡ç´¢å¼•...")
            pdf_vectorstore = FAISS.from_documents(splits, embeddings)
            print("âœ“ å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆ")
            
            return f"""âœ… æˆåŠŸåŠ è½½ä¸­æ–‡PDFæ–‡ä»¶ï¼
ğŸ“Š æ–‡æ¡£ä¿¡æ¯ï¼š
  - æ–‡æ¡£é¡µæ•°: {len(documents)}
  - æ–‡æœ¬å—æ•°: {len(splits)}
  - Embeddingæ¨¡å‹: BAAI/bge-base-zh-v1.5ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰
  - å‘é‡æ•°æ®åº“: FAISS
  
âœ¨ å·²å»ºç«‹å‘é‡ç´¢å¼•ï¼Œå¯ä»¥å¼€å§‹æŸ¥è¯¢åˆ†æè´¢åŠ¡æ•°æ®ï¼"""
            
        except Exception as emb_error:
            return f"""âŒ åˆ›å»ºå‘é‡ç´¢å¼•å¤±è´¥: {str(emb_error)}

ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š
1. è¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–ï¼špip install sentence-transformers
2. é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦400MBï¼‰ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸
3. å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•æ‰‹åŠ¨è®¾ç½®é•œåƒæºæˆ–ä½¿ç”¨ä»£ç†"""
    
    except Exception as e:
        return f"âŒ åŠ è½½PDFæ–‡ä»¶å¤±è´¥: {str(e)}\n\nğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿PDFæ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œä¸”æ–‡ä»¶æœªæŸåã€‚"


def expand_query_with_synonyms(query: str, max_expansion: int = 3) -> list:
    """
    æ‰©å±•æŸ¥è¯¢è¯ï¼Œå¢åŠ è´¢åŠ¡é¢†åŸŸåŒä¹‰è¯/ç›¸å…³è¯ï¼ˆç²¾ç®€ç‰ˆï¼‰
    
    Args:
        query: åŸå§‹æŸ¥è¯¢è¯
        max_expansion: æœ€å¤§æ‰©å±•è¯æ•°é‡ï¼ˆé»˜è®¤3ä¸ªï¼Œé¿å…æœç´¢è¿‡å¤šï¼‰
    
    Returns:
        æ‰©å±•åçš„æŸ¥è¯¢è¯åˆ—è¡¨
    """
    # è´¢åŠ¡é¢†åŸŸåŒä¹‰è¯æ˜ å°„ï¼ˆç²¾ç®€ç‰ˆï¼Œåªä¿ç•™æœ€å¸¸ç”¨çš„å˜ä½“ï¼‰
    financial_synonyms = {
        "åˆ©æ¶¦": ["å‡€åˆ©æ¶¦", "å½’å±äºæ¯å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦"],
        "æ”¶å…¥": ["è¥ä¸šæ”¶å…¥", "è¥ä¸šæ€»æ”¶å…¥"],
        "èµ„äº§": ["æ€»èµ„äº§", "èµ„äº§æ€»è®¡"],
        "è´Ÿå€º": ["æ€»è´Ÿå€º", "è´Ÿå€ºåˆè®¡"],
        "ç°é‡‘æµ": ["ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢"],
        "æ¯›åˆ©": ["æ¯›åˆ©ç‡"],
        "å‡€åˆ©ç‡": ["é”€å”®å‡€åˆ©ç‡"],
        "ROE": ["å‡€èµ„äº§æ”¶ç›Šç‡"],
        "ROA": ["æ€»èµ„äº§æ”¶ç›Šç‡"],
        "EPS": ["æ¯è‚¡æ”¶ç›Š", "åŸºæœ¬æ¯è‚¡æ”¶ç›Š"],
        "è¥æ”¶": ["è¥ä¸šæ”¶å…¥"],
        "æˆæœ¬": ["è¥ä¸šæˆæœ¬"],
        "è´¹ç”¨": ["é”€å”®è´¹ç”¨", "ç®¡ç†è´¹ç”¨", "è´¢åŠ¡è´¹ç”¨"],
    }
    
    queries = [query]
    
    # åªåŒ¹é…ç¬¬ä¸€ä¸ªå‘½ä¸­çš„å…³é”®è¯ï¼Œé¿å…è¿‡åº¦æ‰©å±•
    for key, synonyms in financial_synonyms.items():
        if key in query:
            # åªæ·»åŠ æœ‰é™æ•°é‡çš„åŒä¹‰è¯
            queries.extend(synonyms[:max_expansion])
            break
        for syn in synonyms:
            if syn in query:
                queries.append(key)
                break
    
    # å»é‡å¹¶è¿”å›ï¼Œé™åˆ¶æ€»æ•°é‡
    unique_queries = list(set(queries))
    return unique_queries[:max_expansion + 1]  # åŸå§‹æŸ¥è¯¢ + max_expansion ä¸ªæ‰©å±•è¯


@tool
def search_financial_info(query: str) -> str:
    """
    ä»å·²åŠ è½½çš„è´¢åŠ¡æŠ¥è¡¨PDFä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
    
    Args:
        query: è¦æŸ¥è¯¢çš„è´¢åŠ¡ä¿¡æ¯ï¼ˆå¦‚"è¥ä¸šæ”¶å…¥"ã€"å‡€åˆ©æ¶¦"ã€"èµ„äº§è´Ÿå€ºè¡¨"ã€"å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦"ç­‰ï¼‰
    
    Returns:
        æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯
    """
    global pdf_vectorstore
    
    if pdf_vectorstore is None:
        return "âŒ è¯·å…ˆä½¿ç”¨ load_financial_pdf å·¥å…·åŠ è½½PDFæ–‡ä»¶"
    
    try:
        # æ‰©å±•æŸ¥è¯¢è¯
        expanded_queries = expand_query_with_synonyms(query)
        
        all_docs = []
        seen_contents = set()
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¯è¿›è¡Œæ£€ç´¢
        for q in expanded_queries:
            # ä½¿ç”¨å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢ï¼Œè·å–æ›´å¤šå€™é€‰
            docs_with_scores = pdf_vectorstore.similarity_search_with_score(q, k=5)
            
            for doc, score in docs_with_scores:
                # è¿‡æ»¤ä½ç›¸å…³åº¦ç»“æœï¼ˆåˆ†æ•°è¶Šä½è¶Šç›¸ä¼¼ï¼ŒFAISS ä½¿ç”¨ L2 è·ç¦»ï¼‰
                # åŒæ—¶å»é‡
                content_hash = hash(doc.page_content[:100])
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    all_docs.append((doc, score, q))
        
        if not all_docs:
            return f"æœªæ‰¾åˆ°å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯"
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åºï¼ˆåˆ†æ•°è¶Šä½è¶Šå¥½ï¼‰
        all_docs.sort(key=lambda x: x[1])
        
        # å–å‰5ä¸ªæœ€ç›¸å…³çš„ç»“æœ
        top_docs = all_docs[:5]
        
        # æ•´åˆæ£€ç´¢ç»“æœ
        result = f"ğŸ“„ å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯ï¼š\n\n"
        for i, (doc, score, matched_query) in enumerate(top_docs, 1):
            result += f"ç‰‡æ®µ {i} (åŒ¹é…è¯: {matched_query}, ç›¸å…³åº¦: {1/(1+score):.2%}):\n{doc.page_content}\n\n{'='*50}\n\n"
        
        return result
    
    except Exception as e:
        return f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}"


def extract_number_from_text(text: str) -> list:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰æ•°å­—ï¼ˆæ”¯æŒåƒåˆ†ä½ã€å°æ•°ã€è´Ÿæ•°ã€å¸¦å•ä½ï¼‰
    
    Returns:
        [(æ•°å­—å€¼, åŸå§‹å­—ç¬¦ä¸²), ...]
    """
    # åŒ¹é…å„ç§æ•°å­—æ ¼å¼ï¼šè´Ÿæ•°ã€åƒåˆ†ä½ã€å°æ•°ã€å¸¦å•ä½ï¼ˆä¸‡ã€äº¿ã€å…ƒï¼‰
    patterns = [
        r'(-?[\d,ï¼Œ]+\.?\d*)\s*(?:ä¸‡å…ƒ|äº¿å…ƒ|å…ƒ)?',
        r'(-?[\d,ï¼Œ]+\.?\d*)',
    ]
    
    results = []
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                match = match[0]
            # æ¸…ç†æ ¼å¼
            clean_num = match.replace(',', '').replace('ï¼Œ', '').strip()
            if clean_num and clean_num not in ['-', '.']:
                try:
                    value = float(clean_num)
                    if abs(value) > 0:  # æ’é™¤0
                        results.append((value, match))
                except:
                    pass
    
    # å»é‡
    seen = set()
    unique_results = []
    for value, original in results:
        if value not in seen:
            seen.add(value)
            unique_results.append((value, original))
    
    return unique_results


@tool
def extract_financial_data(data_type: str) -> str:
    """
    ä»PDFä¸­æå–ç‰¹å®šçš„è´¢åŠ¡æ•°æ®ï¼ˆé€šè¿‡ RAG æ£€ç´¢ + æ™ºèƒ½åŒ¹é…ï¼‰
    
    Args:
        data_type: æ•°æ®ç±»å‹ï¼Œå¯é€‰å€¼åŒ…æ‹¬ï¼š
            - 'revenue': è¥ä¸šæ”¶å…¥
            - 'net_income': å‡€åˆ©æ¶¦  
            - 'total_assets': æ€»èµ„äº§
            - 'total_liabilities': æ€»è´Ÿå€º
            - 'equity': è‚¡ä¸œæƒç›Š
            - 'current_assets': æµåŠ¨èµ„äº§
            - 'current_liabilities': æµåŠ¨è´Ÿå€º
            - 'cash': ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©
            - 'operating_income': å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦
            - 'all': æå–æ‰€æœ‰å…³é”®è´¢åŠ¡æŒ‡æ ‡
    
    Returns:
        æå–çš„è´¢åŠ¡æ•°æ®åŠç›¸å…³ä¸Šä¸‹æ–‡
    """
    global pdf_vectorstore, pdf_content
    
    if pdf_vectorstore is None:
        return "âŒ è¯·å…ˆä½¿ç”¨ load_financial_pdf å·¥å…·åŠ è½½PDFæ–‡ä»¶"
    
    # è´¢åŠ¡æŒ‡æ ‡çš„æ£€ç´¢å…³é”®è¯å’Œåˆ«å
    data_config = {
        'revenue': {
            'name': 'è¥ä¸šæ”¶å…¥',
            'keywords': ['è¥ä¸šæ”¶å…¥', 'è¥ä¸šæ€»æ”¶å…¥', 'ä¸»è¥ä¸šåŠ¡æ”¶å…¥', 'ä¸€ã€è¥ä¸šæ”¶å…¥'],
            'patterns': [
                r'(?:ä¸€ã€)?è¥ä¸š(?:æ€»)?æ”¶å…¥[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
                r'è¥ä¸šæ”¶å…¥\s+([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'net_income': {
            'name': 'å‡€åˆ©æ¶¦',
            'keywords': ['å‡€åˆ©æ¶¦', 'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦', 'å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦'],
            'patterns': [
                r'(?:å››ã€)?å‡€åˆ©æ¶¦[^\d]*?([\d,ï¼Œ-]+(?:\.\d+)?)',
                r'å½’å±äº.*?å‡€åˆ©æ¶¦[^\d]*?([\d,ï¼Œ-]+(?:\.\d+)?)',
            ]
        },
        'total_assets': {
            'name': 'æ€»èµ„äº§',
            'keywords': ['èµ„äº§æ€»è®¡', 'èµ„äº§æ€»é¢', 'æ€»èµ„äº§'],
            'patterns': [
                r'èµ„äº§æ€»è®¡[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
                r'æ€»èµ„äº§[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'total_liabilities': {
            'name': 'æ€»è´Ÿå€º',
            'keywords': ['è´Ÿå€ºåˆè®¡', 'è´Ÿå€ºæ€»è®¡', 'è´Ÿå€ºæ€»é¢'],
            'patterns': [
                r'è´Ÿå€º(?:åˆè®¡|æ€»è®¡)[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'equity': {
            'name': 'è‚¡ä¸œæƒç›Š',
            'keywords': ['æ‰€æœ‰è€…æƒç›Šåˆè®¡', 'è‚¡ä¸œæƒç›Šåˆè®¡', 'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…æƒç›Š'],
            'patterns': [
                r'(?:æ‰€æœ‰è€…|è‚¡ä¸œ)æƒç›Š.*?åˆè®¡[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
                r'å½’å±äºæ¯å…¬å¸.*?æƒç›Š[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'current_assets': {
            'name': 'æµåŠ¨èµ„äº§',
            'keywords': ['æµåŠ¨èµ„äº§åˆè®¡', 'æµåŠ¨èµ„äº§å°è®¡'],
            'patterns': [
                r'æµåŠ¨èµ„äº§(?:åˆè®¡|å°è®¡)[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'current_liabilities': {
            'name': 'æµåŠ¨è´Ÿå€º',
            'keywords': ['æµåŠ¨è´Ÿå€ºåˆè®¡', 'æµåŠ¨è´Ÿå€ºå°è®¡'],
            'patterns': [
                r'æµåŠ¨è´Ÿå€º(?:åˆè®¡|å°è®¡)[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'cash': {
            'name': 'è´§å¸èµ„é‡‘',
            'keywords': ['è´§å¸èµ„é‡‘', 'ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©', 'åº“å­˜ç°é‡‘'],
            'patterns': [
                r'è´§å¸èµ„é‡‘[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
                r'ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©[^\d]*?([\d,ï¼Œ]+(?:\.\d+)?)',
            ]
        },
        'operating_income': {
            'name': 'å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦',
            'keywords': ['æ‰£é™¤éç»å¸¸æ€§æŸç›Š', 'æ‰£éå‡€åˆ©æ¶¦', 'æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦'],
            'patterns': [
                r'æ‰£é™¤éç»å¸¸æ€§æŸç›Š.*?å‡€åˆ©æ¶¦[^\d]*?([\d,ï¼Œ-]+(?:\.\d+)?)',
                r'å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„æ‰£é™¤éç»å¸¸æ€§æŸç›Šçš„å‡€åˆ©æ¶¦[^\d]*?([\d,ï¼Œ-]+(?:\.\d+)?)',
            ]
        },
    }
    
    def search_and_extract(config):
        """ä½¿ç”¨ RAG æ£€ç´¢å¹¶æå–æ•°æ®"""
        all_context = []
        extracted_values = []
        
        # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæ£€ç´¢
        for keyword in config['keywords']:
            try:
                docs = pdf_vectorstore.similarity_search(keyword, k=3)
                for doc in docs:
                    content = doc.page_content
                    all_context.append(content)
                    
                    # å°è¯•ç”¨æ­£åˆ™æå–æ•°å€¼
                    for pattern in config['patterns']:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        for match in matches:
                            clean_num = match.replace(',', '').replace('ï¼Œ', '').strip()
                            try:
                                value = float(clean_num)
                                if abs(value) > 0:
                                    extracted_values.append(value)
                            except:
                                pass
            except:
                pass
        
        # è¿”å›æ‰¾åˆ°çš„å€¼ï¼ˆå–æœ€å¤§å€¼ï¼Œé€šå¸¸è´¢åŠ¡æŠ¥è¡¨çš„åˆè®¡æ•°è¾ƒå¤§ï¼‰
        unique_values = list(set(extracted_values))
        unique_values.sort(reverse=True)
        
        return unique_values, all_context
    
    if data_type == 'all':
        # æå–æ‰€æœ‰æŒ‡æ ‡
        result = "ğŸ“Š æå–çš„è´¢åŠ¡æ•°æ®ï¼š\n\n"
        extracted_data = {}
        
        for key, config in data_config.items():
            values, contexts = search_and_extract(config)
            if values:
                # å–æœ€å¯èƒ½çš„å€¼ï¼ˆé€šå¸¸æ˜¯æœ€å¤§çš„ï¼‰
                value = values[0]
                extracted_data[key] = value
                result += f"- {config['name']}: {value:,.2f}\n"
            else:
                result += f"- {config['name']}: æœªæ‰¾åˆ°\n"
        
        # å¦‚æœæå–åˆ°æ•°æ®è¾ƒå°‘ï¼Œé™„åŠ åŸå§‹ä¸Šä¸‹æ–‡ä¾› LLM åˆ†æ
        found_count = len([v for v in extracted_data.values() if v])
        if found_count < 5:
            result += "\n\nâš ï¸ éƒ¨åˆ†æ•°æ®æœªèƒ½è‡ªåŠ¨æå–ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³åŸå§‹å†…å®¹ä¾›åˆ†æï¼š\n\n"
            # æ£€ç´¢ä¸»è¦è´¢åŠ¡æŠ¥è¡¨åŒºåŸŸ
            for keyword in ['åˆ©æ¶¦è¡¨', 'èµ„äº§è´Ÿå€ºè¡¨', 'ä¸»è¦ä¼šè®¡æ•°æ®']:
                try:
                    docs = pdf_vectorstore.similarity_search(keyword, k=2)
                    for doc in docs:
                        result += f"---\n{doc.page_content[:500]}\n"
                except:
                    pass
        
        return result
    
    elif data_type in data_config:
        config = data_config[data_type]
        values, contexts = search_and_extract(config)
        
        if values:
            result = f"ğŸ“Š {config['name']}: {values[0]:,.2f}\n"
            if len(values) > 1:
                result += f"   (å…¶ä»–å€™é€‰å€¼: {', '.join([f'{v:,.2f}' for v in values[1:3]])})\n"
            result += f"\nç›¸å…³ä¸Šä¸‹æ–‡:\n{contexts[0][:300] if contexts else 'æ— '}..."
            return result
        else:
            # è¿”å›æ£€ç´¢åˆ°çš„åŸå§‹å†…å®¹ï¼Œè®© LLM è‡ªè¡Œåˆ†æ
            result = f"â“ æœªèƒ½è‡ªåŠ¨æå– {config['name']}ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³å†…å®¹ï¼š\n\n"
            for keyword in config['keywords'][:2]:
                try:
                    docs = pdf_vectorstore.similarity_search(keyword, k=2)
                    for doc in docs:
                        result += f"---\n{doc.page_content[:400]}\n"
                except:
                    pass
            return result
    
    else:
        return f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}"


def create_financial_agent():
    """åˆ›å»ºè´¢åŠ¡åˆ†ææ™ºèƒ½ä½“"""
    
    # åˆå§‹åŒ– DeepSeek æ¨¡å‹
    # è¯´æ˜ï¼šDeepSeek æä¾› OpenAI å…¼å®¹çš„ APIï¼Œæ‰€ä»¥ä½¿ç”¨ ChatOpenAI ç±»
    # åªéœ€å°† openai_api_base è®¾ç½®ä¸º DeepSeek çš„ API åœ°å€å³å¯
    llm = ChatOpenAI(
        model="deepseek-chat",
        openai_api_key=DEEPSEEK_API_KEY,  # ä½¿ç”¨ DeepSeek API Key
        openai_api_base="https://api.deepseek.com",  # DeepSeek API åœ°å€
        temperature=0,
    )
    
    # å®šä¹‰å·¥å…·åˆ—è¡¨
    tools = [
        load_financial_pdf,
        # search_financial_info,
        extract_financial_data,
        calculate_financial_ratio,
        analyze_profitability,
        analyze_liquidity,
        analyze_leverage,
    ]
    
    # åˆ›å»ºå†…å­˜ä¿å­˜å™¨
    memory = MemorySaver()
    
    # åˆ›å»ºç³»ç»Ÿæç¤ºï¼ˆä½¿ç”¨ SystemMessage å¯¹è±¡ï¼‰
    system_message = SystemMessage(content="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è´¢åŠ¡åˆ†æå¸ˆåŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æä¼ä¸šè´¢åŠ¡æŠ¥è¡¨ã€‚
`
ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
1. åŠ è½½å’Œè¯»å–PDFæ ¼å¼çš„è´¢åŠ¡æŠ¥è¡¨
2. ä»è´¢åŠ¡æŠ¥è¡¨ä¸­æå–å…³é”®è´¢åŠ¡æ•°æ®
3. è®¡ç®—å„ç§è´¢åŠ¡æ¯”ç‡ï¼ˆå¦‚ ROEã€ROAã€æµåŠ¨æ¯”ç‡ç­‰ï¼‰
4. åˆ†æä¼ä¸šçš„ç›ˆåˆ©èƒ½åŠ›
7. æä¾›ä¸“ä¸šçš„è´¢åŠ¡å»ºè®®
8. æä¾›çœŸå®å®¢è§‚çš„åˆ†æï¼Œä¸èƒ½æ•…æ„è¯´å¥½è¯

å¯ç”¨å·¥å…·è¯´æ˜ï¼š
- load_financial_pdf: åŠ è½½PDFè´¢åŠ¡æŠ¥è¡¨æ–‡ä»¶
- search_financial_info: ä»PDFä¸­æ£€ç´¢ç‰¹å®šä¿¡æ¯ï¼ˆæ¨èä½¿ç”¨ï¼‰/
- extract_financial_data: è‡ªåŠ¨æå–è´¢åŠ¡æ•°æ®ï¼ˆè¥ä¸šæ”¶å…¥ã€å‡€åˆ©æ¶¦ç­‰ï¼‰
- calculate_financial_ratio: è®¡ç®—è´¢åŠ¡æ¯”ç‡
- analyze_profitability: åˆ†æç›ˆåˆ©èƒ½åŠ›
- analyze_liquidity: åˆ†ææµåŠ¨æ€§
- analyze_leverage: åˆ†ææ æ†

âš ï¸ æ•°æ®æå–ç­–ç•¥ï¼ˆé‡è¦ï¼‰ï¼š
1. ä¼˜å…ˆä½¿ç”¨ search_financial_info æ£€ç´¢åŸå§‹å†…å®¹ï¼Œç„¶åè‡ªå·±ä»è¿”å›çš„æ–‡æœ¬ä¸­è¯†åˆ«æ•°å­—
2. å¦‚æœ extract_financial_data è¿”å›"æœªæ‰¾åˆ°"ï¼Œä¸è¦æ”¾å¼ƒï¼ä½¿ç”¨ search_financial_info æœç´¢ç›¸å…³å…³é”®è¯
3. è´¢åŠ¡æŠ¥è¡¨ä¸­çš„æ•°å­—å¯èƒ½ä»¥ä¸åŒæ ¼å¼å‡ºç°ï¼š
   - è¡¨æ ¼å½¢å¼ï¼šæŒ‡æ ‡åç§°å’Œæ•°å­—å¯èƒ½åœ¨ä¸åŒè¡Œ
   - åƒåˆ†ä½åˆ†éš”ï¼šå¦‚ 1,234,567.89
   - å•ä½æ ‡æ³¨ï¼šå¦‚ "å•ä½ï¼šä¸‡å…ƒ" æˆ– "å•ä½ï¼šå…ƒ"
4. ä»ä¸Šä¸‹æ–‡æ¨æ–­ï¼šå¦‚æœçœ‹åˆ° "å•ä½ï¼šä¸‡å…ƒ"ï¼Œæ•°å­—éœ€è¦ä¹˜ä»¥ 10000

å·¥ä½œæµç¨‹ï¼š
1. åŠ è½½PDFæ–‡ä»¶
2. ä½¿ç”¨ extract_financial_data('all') å°è¯•è‡ªåŠ¨æå–
3. å¯¹äºæœªæå–åˆ°çš„æ•°æ®ï¼Œä½¿ç”¨ search_financial_info æœç´¢ï¼Œä¾‹å¦‚ï¼š
   - search_financial_info("å‡€åˆ©æ¶¦")
   - search_financial_info("èµ„äº§è´Ÿå€ºè¡¨")
   - search_financial_info("åˆ©æ¶¦è¡¨")
5. åŸºäºæ”¶é›†åˆ°çš„æ•°æ®è¿›è¡Œåˆ†æ

âš ï¸ é‡è¦è§„åˆ™ï¼š
- ä¸è¦è½»æ˜“è¯´"æ— æ³•åˆ†æ"ï¼Œè¦åŠªåŠ›ä»åŸå§‹æ–‡æœ¬ä¸­æå–ä¿¡æ¯
- å¦‚æœå·¥å…·è¿”å›äº†åŸå§‹æ–‡æœ¬ï¼Œä»”ç»†é˜…è¯»å¹¶ä»ä¸­æå–æ•°å­—
- æ³¨æ„å•ä½æ¢ç®—ï¼ˆä¸‡å…ƒã€äº¿å…ƒï¼‰
- ä½¿ç”¨ä¸­æ–‡å›ç­”
- æä¾›å®¢è§‚çœŸå®çš„åˆ†æ

å¦‚æœç”¨æˆ·æä¾›äº†è´¢åŠ¡æ•°æ®æˆ–PDFæ–‡ä»¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„å…·ä½“è¦æ±‚ä½¿ç”¨ç›¸åº”çš„å·¥å…·ã€‚""")
    
    # åˆ›å»º ReAct agent
    agent = create_react_agent(llm, tools, checkpointer=memory)
    
    return agent, system_message


def main(pdf_path):
    """è¿è¡Œå¸¦PDFåˆ†æçš„ç¤ºä¾‹ - æµå¼ç‰ˆæœ¬"""
    print("="*60)
    print("ğŸ¢ è´¢åŠ¡æŠ¥è¡¨PDFåˆ†æç¤ºä¾‹")
    print("="*60)
    
    # åˆ›å»º agent
    agent, system_message = create_financial_agent()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        f"è¯·åŠ è½½è¿™ä¸ªPDFæ–‡ä»¶ï¼š{pdf_path},åˆ†æè¿™å®¶å…¬å¸çš„æ•´ä½“è´¢åŠ¡çŠ¶å†µ",
    ]
    
    thread_id = "pdf_analysis_session"
    config = {
        "configurable": {"thread_id": thread_id},
        "recursion_limit": 100000
    }
    
    for i, query in enumerate(test_queries, 1):
        # ç¬¬ä¸€æ¬¡å¯¹è¯æ—¶åŒ…å«ç³»ç»Ÿæ¶ˆæ¯
        if i == 1:
            messages = [system_message, HumanMessage(content=query)]
        else:
            messages = [HumanMessage(content=query)]

        # è¿”å›æµ
        stream = agent.invoke(
            {"messages": messages},
            config=config
        )
        return stream.content
  


def main_with_pdf(pdf_path: str) -> Generator:
    """è¿è¡Œå¸¦PDFåˆ†æçš„ç¤ºä¾‹ - æµå¼ç‰ˆæœ¬"""
    print("="*60)
    print("ğŸ¢ è´¢åŠ¡æŠ¥è¡¨PDFåˆ†æç¤ºä¾‹")
    print("="*60)
    
    # åˆ›å»º agent
    agent, system_message = create_financial_agent()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        f"è¯·åŠ è½½è¿™ä¸ªPDFæ–‡ä»¶ï¼š{pdf_path}",
        "ä»PDFä¸­æå–æ‰€æœ‰å…³é”®è´¢åŠ¡æ•°æ®",
        "åŸºäºæå–çš„æ•°æ®ï¼Œåˆ†æè¿™å®¶å…¬å¸çš„æ•´ä½“è´¢åŠ¡çŠ¶å†µ",
    ]
    
    thread_id = "pdf_analysis_session"
    config = {
        "configurable": {"thread_id": thread_id},
        "recursion_limit": 100000
    }
    
    for i, query in enumerate(test_queries, 1):
        # ç¬¬ä¸€æ¬¡å¯¹è¯æ—¶åŒ…å«ç³»ç»Ÿæ¶ˆæ¯
        if i == 1:
            messages = [system_message, HumanMessage(content=query)]
        else:
            messages = [HumanMessage(content=query)]

        # è¿”å›æµ
        stream = agent.stream(
            {"messages": messages},
            config=config,
            stream_mode="values"
        )
        
        # ä½¿ç”¨ç”Ÿæˆå™¨é€ä¸ªäº§ç”Ÿäº‹ä»¶
        for chunk in stream:
            latest_message = chunk["messages"][-1]
            
            if latest_message.content:
                yield {
                    "type": "message",
                    "step": i,
                    "content": latest_message.content
                }
            elif hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
                tools = [tc['name'] for tc in latest_message.tool_calls]
                yield {
                    "type": "tool_call",
                    "step": i,
                    "tools": tools
                }
    
    # åˆ†æå®Œæˆ
    yield {
        "type": "complete",
        "message": "åˆ†æå®Œæˆ"
    }

if __name__ == "__main__":
    pdf_path = "pdf/000001.pdf"
    print(main(pdf_path))